---
title: 'STAT 413/613 Homework on Web Data: APIs and Scraping'
author: "Jaehee Lee"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  pdf_document:
    number_sections: yes
    toc: no
    toc_depth: '4'
urlcolor: blue

---

# Instructions {-}

- Write your solutions **in this starter file**. 
  + Modify the "author" field in the YAML header.
- Commit R Markdown and HTML files (no PDF files). **Push both .Rmd and HTML files to GitHub**.
  + Make sure you have knitted to HTML for your final submission.
- **Only include necessary code and data** to answer the questions.
- Most of the functions you use should be from the tidyverse. **Too much base R **will result in point deductions.
- Submit a response on Canvas that your assignment is complete on GitHub
- Feel free to use Pull requests and or email (attach your .Rmd) to ask me any questions.

# IMDB List of Oscar Winners

IMDB has a list of the [Oscar Best Picture Winners](https://www.imdb.com/search/title/?count=100&groups=oscar_best_picture_winners&sort=year%2Cdesc&ref_=nv_ch_osc).

Scrape the following elements, convert the data into a tibble, tidy it, and clean it to answer the questions below: 

- Number
- Title
- Year
- MPAA Rating
- Length in minutes
- Genre
- Star Rating
- Metascore Rating
- Gross Receipts

Convert the data into a tibble, tidy it, and clean it to answer the following questions:

```{r}
# load packages
library(rvest)
library(tidyverse)
library(stringr)
```

```{r}
# html_obj <- read_html("https://www.imdb.com/search/title/?count=100&groups=oscar_best_picture_winners&sort=year%2Cdesc&ref_=nv_ch_osc")

# I used below html url because I am in South Korea and when I use the above url, some of the data are translated and some of them appear different, so I figured out using cache.
```


```{r}
# Use read_html() to save an HTML file to a variable.
html_obj_1 <- read_html("https://webcache.googleusercontent.com/search?q=cache:fvpH0ObPBr0J:https://www.imdb.com/search/title/%3Fcount%3D100%26groups%3Doscar_best_picture_winners%26sort%3Dyear%252Cdesc%26ref_%3Dnv_ch_osc+&cd=1&hl=ko&ct=clnk&gl=kr")
class(html_obj_1)
```

```{r}
Data <- html_nodes(html_obj_1,
                              css = ".ghost~ .text-muted+ span , .genre , .runtime ,
.certificate , strong , .favorable , .unbold , .lister-item-header a")

Data_text <- html_text(Data)
```


```{r}
datadf <- tibble(text = Data_text)

datadf %>%
  mutate(ismovierank = str_detect(text, "^\\d+\\.$")) -> datadf

datadf %>%
  mutate(movienum = cumsum(ismovierank)) %>%
  filter(movienum > 0) -> datadf

datadf %>%
  mutate(isyear = str_detect(text, "\\(\\d{4}\\)")) -> datadf

datadf %>%
  mutate(isgenre = str_detect(text, "^\\n")) -> datadf

datadf %>% 
  mutate(isStarRating = str_detect(text, "^\\d+\\.+\\d$")) -> datadf

datadf %>% 
  mutate(isGross_Receipts = str_detect(text, "^\\$")) -> datadf

data0 <- html_nodes(html_obj_1, ".lister-item-header a")
data0_text <- html_text(data0)
datadf %>%
  mutate(isname = text %in% data0_text) -> datadf

datadf %>%  
  mutate(islength = str_detect(text,"^\\d{1,3}+\\s+min$")) -> datadf

datadf %>%
  mutate(isMetascore_Rating = str_detect(text, "^\\d{1,3}+\\s+$")) -> datadf

datadf %>%
  mutate(isMPAA = !ismovierank & !isyear & !isgenre & !isStarRating & !isGross_Receipts & !isname &!isMetascore_Rating &!islength) ->datadf
head(datadf)
## make sure we have 93 movies:
sum(datadf$isname)
```

```{r}
datadf %>%
  mutate(key = case_when(ismovierank ~ "rank",
                         isyear ~ "year",
                         isgenre ~ "genre",
                        islength ~ "length",
                         isMetascore_Rating ~ "metacritic", 
                         isGross_Receipts ~ "gross_receipts", 
                         isStarRating ~ "star_rating", 
                        isMPAA ~ "MPAA",
                        isname ~ "name")) %>%
  select(key, text, movienum) %>%
  pivot_wider(names_from = key, values_from = text) -> datawide
head(datawide)
```
```{r}
datawide %>%
  mutate(length = str_extract_all(length, "[0-9]+")) %>% 
  mutate(year = str_extract_all(year, "[0-9]+")) %>% 
  mutate(gross_receipts = str_sub(gross_receipts, 2, -2)) %>% 
  mutate(genre = str_replace_all(genre, "\\n", ""),
         genre = str_squish(genre)) %>% 
  mutate(movienum=NULL) -> datawide
head(datawide)
```
```{r}
# change the datatype 
datawide$year <- as.numeric(datawide$year)
datawide$length <- as.numeric(datawide$length)
datawide$star_rating <- as.numeric(datawide$star_rating)
datawide$metacritic <- as.double(datawide$metacritic)
datawide$gross_receipts <- as.numeric(datawide$gross_receipts)
```

```{r}
# see the summary and str of the data
str(datawide)
summary(datawide)
```

1. Show a summary of the number of data elements for each movie that are not NA. Which two elements are missing the most from the movies?
```{r}
# 1 method
datawide %>% 
  summarise_all(funs(sum(!is.na(.))))
# 2 method
sum(is.na(datawide$rank))
sum(is.na(datawide$name))
sum(is.na(datawide$MPAA))
sum(is.na(datawide$length))
sum(is.na(datawide$genre))
sum(is.na(datawide$star_rating))
sum(is.na(datawide$metacritic))
sum(is.na(datawide$gross_receipts))
# The two elements that are missing the most from the movies: metaScore_Rating and gross_receipts
```

2. Create a plot of the length of a film and its gross, color coded by rating. Show linear smoothers for each rating.
- Does MPAA rating matter?  
```{r}
library(ggplot2)
sum(datadf$islength)
sum(datadf$isGross_Receipts)
```
```{r}
ggplot(data = datawide, aes(x = length, y = gross_receipts, color = MPAA)) +
   geom_smooth(method = "lm", se = FALSE)+
   geom_point()
```
- Does MPAA rating matter?
Yes and No. When MPAA is "PG-13", length and gross_receipts have a positive correlation, so MPAA rating matters. Also when MPAA is "Passed", length and gross_receipts have a positive correlation, so MPAA rating matters. However, other MPAA show that the lines are almost horizontal to x-axis, therefore, there is no correlation between length and gross_receipts, so it doesn't matter for other MPAA.


3. Create a plot with a single Ordinary Least Squares smoothing line with no standard errors showing for predicting stars rating based on metacritic scores for those movies that have metacritic scores. 
- Use a linear model to assess if there is there a meaningful relationship. Show the summary of the output and interpret in terms of the $p$-value and the adjusted R-Squared?
```{r}
datawide %>% 
  ggplot(aes(y = star_rating, x = metacritic)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
data_3 <- lm(metacritic ~ star_rating, data = datawide)
summary(data_3)
```
Interpretation: 
p-value is 0.007443 which is smaller than significance level( =0.01),this will reject the null hypothesis and this tells us that this is statistically significant. Adusted R-squared is 0.08059, this means that we can only explain 8% of this model, indicating that this is not a good model. Since Adjusted R2 indicates how well terms fit a curve or line. When we see the graph above, we can easily see that a lot of points do not well align with the blue line. Therefore, I feel this is not a good model: This linear model doesn't show that there is a meaningful relationship between star_rating and metaScore_rating. 

4. Use an appropriate plot to compare the gross receipts by MPAA rating.
```{r}
str(datawide)
datawide %>%  
  group_by(as.factor(MPAA)) %>% 
ggplot(aes(y = gross_receipts, x = MPAA)) +
  geom_boxplot() 
```

  + Which MPAA rating has the highest median gross receipts? 
```{r}
# The black line inside the box is the median. We can see that when MPAA is "PG-13", the median gross_receipts is the highest.
```
  
  + Which R-rated movies are in the overall top 10 of gross receipts?
```{r}
datawide %>%  
  filter(MPAA == "R") %>%  
  arrange(desc(gross_receipts)) %>%  
  head(10) %>%  
  select(name)
```
  
  + Use one-way analysis of variance to assess the level of evidence for whether all ratings have the same mean gross receipts. Show the summary of the results and provide your interpretation of the results.
```{r}
# Compute the analysis of variance
one.way <- aov(gross_receipts ~ MPAA, data = datawide)
# Summary of the analysis
summary(one.way)
```
Interpretation: 
As the p-value is bigger than the significance level 0.05, we can conclude that there are not significant differences between the groups.

```{r}
library(tidyverse)
library(keyring)
library(httr)
```

```{r}
#key_set("API_KEY_SECURE")
#c1hgcef48v6ptfsu4iqg
```

- Pick a website of your choice (not discussed in class) that requires a free API key to download a data set. Convert elements of interest into a tibble and create a graph to answer a question of interest.
```{r}
mout <- GET(url = "https://finnhub.io/api/v1/stock/candle/?c1hgcef48v6ptfsu4iqg",
            query = list(symbol="AAPL",
                         resolution=1,
                         from=1615298999, 
                         to=1615302599,
                         r = "json",
                         token = 'c1hgcef48v6ptfsu4iqg'))
                        #  token = key_get("API_KEY_SECURE")))
content(mout) -> a
```

```{r}
status_code(mout)
```

- State the question and interpret the plot.

# Question: Find out close_price changes against time using ggplot. 
```{r}
time <- a[["t"]]
close <- a[["c"]]
```

```{r}
time %>%  map_dbl(~.) -> time_1

close %>% map_dbl(~.) -> close_1

df <- map2_dfr(time_1, close_1, ~ tibble(time_1 = .x, close_1 = .y)) 
```

```{r}
df %>%  
ggplot(aes(y = close_1, x = time_1)) +
 geom_line()+
 theme_bw()+
  xlab("Time")+
  ylab("Close Price")
```


# Interepretation: 

When time is around 1615300000, the close price is the lowest. Overall the close price is getting higher when the time goes. 

# 3 Extra Credit 1 Pts
Listen to the AI Today podcast on Machine Learning Ops and provide your thoughts on the following questions:

# Does knowing about Git and GitHub help you in understanding the podcast?
[Yes, knowing about Git and GitHub helped me in understanding the podcast. Speaker mentioned Git and GitHub at the beginning of the podcast saying that it is a version control system and it is used for collaborations. He also mentioned Git and GitHub at the end of this podcast. Github is doing commit, push, version control, pushing changes, practices of using version control, iteration on a model, training model. If I didn’t have an experience on Git and Github then it would be hard for me to understand it.]


# How do you think the ideas of ML OPs will affect your future data science projects?
[ML OPs stands for “Machine learning Model Operationalization Management”, which is model development and machine learning model, operationalization and deployment. Speaker explains ML OPs is a new term that is an intersection of software engineering, machine learning, and devOps. Machine learning operatize AI, such as chatbots interacting with customers. I think ML OPs will affect the future data science projects in many different ways. Since these days, a lot of disciplines are merging, a lot of teams of people are using ML in products. As the speaker said, a lot of companies are using it with products. He talks about challenges when using ML models in production. ML OPs will affect future data science projects because ML OPs is not just programming codes, it is coding with data. It is trained by a lot of data which will affect the model. Since, these days massive data is created everyday, in the future there will be a lot of ML OPs usage with data science projects.]


